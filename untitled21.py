# -*- coding: utf-8 -*-
"""Untitled21.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Z5fgjK35O5R1GDVgXiKx-6y39zS1Qkto
"""

!pip install -q unsloth peft transformers accelerate bitsandbytes

from unsloth import FastLanguageModel
from unsloth.trainer import SFTTrainer
from transformers import TrainingArguments
from datasets import load_dataset

# Step 1: Load dataset
dataset = load_dataset("json", data_files="testcase.jsonl", split="train")

# Step 2: Format prompt + completion into a single "text" field
def formatting_func(example):
    return {
        "text": f"{example['prompt'].strip()}\n{example['completion'].strip()}"
    }

dataset = dataset.map(formatting_func)

# Step 3: Load quantized model with device_map for low-VRAM GPUs (like T4)
model, tokenizer = FastLanguageModel.from_pretrained(
    model_name = "unsloth/mistral-7b-instruct-v0.2-bnb-4bit",
    max_seq_length = 1024,
    dtype = None,
    load_in_4bit = True,
    device_map = "auto",  # âœ… enables automatic CPU/GPU memory management
)

# Step 4: Add LoRA adapters for training
model = FastLanguageModel.get_peft_model(
    model,
    r = 16,
    lora_alpha = 32,
    lora_dropout = 0.0,  # âœ… 0.0 for faster Unsloth patching
    bias = "none",
)

# Step 5: Define training configuration
training_args = TrainingArguments(
    output_dir = "finetuned-qagenie",
    num_train_epochs = 3,
    per_device_train_batch_size = 1,
    gradient_accumulation_steps = 1,
    logging_steps = 10,
    save_strategy = "epoch",
    learning_rate = 2e-4,
    bf16 = False,
    fp16 = True,
    optim = "adamw_8bit",
    lr_scheduler_type = "cosine",
    warmup_ratio = 0.1,
    save_total_limit = 2,
)

# Step 6: Initialize trainer (formatting already applied to dataset)
trainer = SFTTrainer(
    model = model,
    tokenizer = tokenizer,
    train_dataset = dataset,
    args = training_args,
)

# Step 7: Start fine-tuning
trainer.train()

from unsloth import FastLanguageModel

# âœ… Load the latest saved checkpoint (with adapter)
model, tokenizer = FastLanguageModel.from_pretrained(
    model_name = "finetuned-qagenie/checkpoint-99",  # ðŸ‘ˆ points to the adapter!
    max_seq_length = 1024,
    dtype = None,
    load_in_4bit = True,
    device_map = "auto",
)

# âœ… Merge LoRA adapter into the base model
model = model.merge_and_unload()

# âœ… Save the fully merged model to disk
model.save_pretrained("merged-qagenie")
tokenizer.save_pretrained("merged-qagenie")

!zip -r merged-qagenie.zip merged-qagenie



from google.colab import drive
drive.mount('/content/drive')